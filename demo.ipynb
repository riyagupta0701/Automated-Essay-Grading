{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.index_to_key)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model[word])        \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = 'word2vecmodel.bin'\n",
    "w2v_model = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./saved_model/\n"
     ]
    }
   ],
   "source": [
    "output_dir = './saved_model/'\n",
    "\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "model_loaded = BertForSequenceClassification.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essay_to_sentences(essay):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(essay.strip())\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker(sent):\n",
    "    tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "    model_loaded = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 64,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "    input_id = encoded_dict['input_ids']\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    input_id = torch.LongTensor(input_id)\n",
    "    attention_mask = torch.LongTensor(attention_mask)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_loaded = model_loaded.to(device)\n",
    "    input_id = input_id.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model_loaded(input_id, token_type_ids=None, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "    index = logits.argmax()\n",
    "    if index == 1:\n",
    "        return 1\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammar_check(essay):\n",
    "    sentences = essay_to_sentences(essay)\n",
    "    n= len(sentences)\n",
    "    s=0\n",
    "    for sentence in sentences:\n",
    "        s+=checker(sentence)\n",
    "    return (s/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear Local newspaper, I think that people have lately spent too much time on their computers. People who have been doing so are increasing the percentage of obecity. The reason I am writing this to you is because the percentage of obecity is currently increasing, people are spending less time exercising and spending less time interacting with others. First, the percentage of obecity is increasing. The reason is that people just stay indoors, they never feel the need to be fit. This is also risking their lives, all they do is just sit and eat. This could risk the chances of getting deceases.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Essay = \"Dear Local newspaper, I think that people have lately spent too much time on their computers. People who have been doing so are increasing the percentage of obecity. The reason I am writing this to you is because the percentage of obecity is currently increasing, people are spending less time exercising and spending less time interacting with others. First, the percentage of obecity is increasing. The reason is that people just stay indoors, they never feel the need to be fit. This is also risking their lives, all they do is just sit and eat. This could risk the chances of getting deceases.\"\n",
    "Essay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grammer Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammer Score :  1.0\n",
      "Processing time: 0:00:09.051919\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "t0 = datetime.now()\n",
    "grammar_score=grammar_check(Essay)\n",
    "print(\"Grammer Score : \", grammar_score)\n",
    "t1 = datetime.now()\n",
    "print('Processing time: {}'.format(t1 - t0))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance(Essay):\n",
    "    wordlist = essay_to_wordlist( Essay, remove_stopwords=True )\n",
    "    essayvec = getAvgFeatureVecs( [wordlist], w2v_model, 300 )\n",
    "    essayvec = np.array(essayvec)\n",
    "    essayvec = np.reshape(essayvec, (essayvec.shape[0], 1, essayvec.shape[1]))\n",
    "    lstm_model =tf.keras.models.load_model(\"./model_weights/final_lstm.h5\")\n",
    "    preds = lstm_model.predict(essayvec)\n",
    "    preds = np.around(preds)\n",
    "    return preds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Score :  8.0\n"
     ]
    }
   ],
   "source": [
    "relevance_score = relevance(Essay)\n",
    "print(\"Relevance Score : \", relevance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Score :  7.482624107250408\n"
     ]
    }
   ],
   "source": [
    "filename = 'finalized_model.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "X=[]\n",
    "X.append(relevance_score)\n",
    "X.append(grammar_score)\n",
    "result = loaded_model.predict([X])\n",
    "print(\"Final Score : \", result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
